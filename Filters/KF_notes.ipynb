{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KF Basics (Notes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the need to describe belief in terms of PDF's?\n",
    "This is because robot environments are stochastic. A robot environment may have cows with Tesla by side. That is a robot and it's environment cannot be deterministically modelled(e.g as a function of something like time t.). In the real world sensors are also error prone, and hence there'll be a set of values with a mean and variance that it can take. Hence, we always have to model around some mean and variances associated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the Expectation of a Random Variables?\n",
    " Expectation is nothing but an average of the probabilites\n",
    " \n",
    "$$\\mathbb E[X] = \\sum_{i=1}^n p_ix_i$$\n",
    "\n",
    "In the continous form,\n",
    "\n",
    "$$\\mathbb E[X] = \\int_{-\\infty}^\\infty x\\, f(x) \\,dx$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4000000000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "x=[3,1,2]\n",
    "p=[0.1,0.3,0.4]\n",
    "E_x=np.sum(np.multiply(x,p))\n",
    "print(E_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the advantage of representing the belief as a unimodal as opposed to multimodal?\n",
    "Obviously, it makes sense because we can't multiple probabilities to a car moving for two locations. This would be too confusing and the information will not be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance, Covariance and Correlation\n",
    "\n",
    "### Variance\n",
    "Variance is the spread of the data. The mean does'nt tell much **about** the data. Therefore the variance tells us about the **story** about the data meaning the spread of the data.\n",
    "\n",
    "$$\\mathit{VAR}(X) = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2004065275190503"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.random.randn(10)\n",
    "np.var(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance\n",
    "\n",
    "This is for a multivariate distribution. For example, a robot in 2-D space can take values in both x and y. To describe them, a normal distribution with mean in both x and y is needed.\n",
    "\n",
    "For a multivariate distribution, mean $\\mu can be represented as a matrix, \n",
    "\n",
    "$$\n",
    "\\mu = \\begin{bmatrix}\\mu_1\\\\\\mu_2\\\\ \\vdots \\\\\\mu_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Similarly, variance can also be represented.\n",
    "\n",
    "But an important concept is that in the same way as every variable or dimension has a variation in its values, it is also possible that there will be values on how they **together vary**. This is also a measure of how two datasets are related to each other or **correlation**.\n",
    "\n",
    "For example, as height increases weight also generally increases. These variables are correlated. They are positively correlated because as one variable gets larger so does the other.\n",
    "\n",
    "We use a **covariance matrix** to denote covariances of a multivariate normal distribution:\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "  \\sigma_1^2 & \\sigma_{12} & \\cdots & \\sigma_{1n} \\\\\n",
    "  \\sigma_{21} &\\sigma_2^2 & \\cdots & \\sigma_{2n} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  \\sigma_{n1} & \\sigma_{n2} & \\cdots & \\sigma_n^2\n",
    " \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Diagonal** - Variance of each variable associated. \n",
    "\n",
    "**Off-Diagonal** - covariance between ith and jth variables.\n",
    "\n",
    "$$\\begin{aligned}VAR(X) = \\sigma_x^2 &=  \\frac{1}{n}\\sum_{i=1}^n(X - \\mu)^2\\\\\n",
    "COV(X, Y) = \\sigma_{xy} &= \\frac{1}{n}\\sum_{i=1}^n[(X-\\mu_x)(Y-\\mu_y)\\big]\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09045511, -0.0505224 , -0.05754722],\n",
       "       [-0.0505224 ,  0.02835353,  0.03559742],\n",
       "       [-0.05754722,  0.03559742,  0.12506988]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.random.random((3,3))\n",
    "np.cov(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "1. Roger Labbe's [repo](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python) on Kalman Filters. (Majority of the examples in the notes are from this)\n",
    "\n",
    "\n",
    "\n",
    "2. Probabilistic Robotics by Sebastian Thrun, Wolfram Burgard and Dieter Fox, MIT Press.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
